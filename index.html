<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/html" xmlns="http://www.w3.org/1999/html"
      xmlns="http://www.w3.org/1999/html" xmlns="http://www.w3.org/1999/html">
<head>
    <center>
        <img src="./static/my_images/logo.jpg" width="521" height="149"
             alt="TextAtlas"/>
        <br/>
    </center>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="https://kit.fontawesome.com/f8ddf9854a.js" crossorigin="anonymous"></script>
    <meta charset="utf-8">
    <meta name="description"
          content="A Large-scale Dataset for Dense Text Image Generation">
    <title>A Large-scale Dataset for Dense Text Image Generation</title>


    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <script src="https://kit.fontawesome.com/fff5b27ec1.js" crossorigin="anonymous"></script>
    <!-- <script src="https://kit.fontawesome.com/eaf1856e6f.js" crossorigin="anonymous"></script> -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>
<body>


    <style>
        .text-block {
            background: #f9f9f9;
            padding: 20px;
            border-left: 6px solid #007BFF;
            border-radius: 8px;
            text-align: left;
            box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1);
            margin: 0 auto;
            margin-bottom: 30px;
            max-width: 900px;
        }

        .highlight {
            font-weight: bold;
            color: #d63384;
        }

        .key-feature {
            font-weight: bold;
            color: #0d6efd;
        }

        .challenge {
            font-weight: bold;
            color: #dc3545;
        }
    </style>
    

<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h2 class="subtitle is-3 publication-subtitle">
                        A Large-scale Dataset for Dense Text Image Generation
                    </h2>
                    <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href=""
                     style="text-decoration: none; color: inherit;"></a>
                </span>
                        <span class="author-block">
                  <a href="https://openreview.net/profile?id=~Alex_Jinpeng_Wang1"
                     style="text-decoration: none; color: inherit;">Alex Jinpeng Wang</a>,
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=RLVSYY0AAAAJ&hl=en"
                     style="text-decoration: none; color: inherit;">Dongxing Mao</a>,
                        <span class="author-block">
                  Jiawei Zhang,
                </span>
                </span>
                        <br>
                        <span class="author-block">Weiming Han,</span>
                        <span class="author-block">Zhuobai Dong,</span>
                         <a href="https://scholar.google.com/citations?user=WR875gYAAAAJ&hl=en"
                     style="text-decoration: none; color: inherit;"><span class="author-block">Linjie Li,</span></a>
                        <span class="author-block"><a href="https://linyq17.github.io"
                     style="text-decoration: none; color: inherit;"><span class="author-block">Yiqi Lin,</span></a>
                        <span class="author-block"><a href="https://scholar.google.com/citations?user=rP02ve8AAAAJ&hl=en"
                     style="text-decoration: none; color: inherit;"><span class="author-block">Zhengyuan Yang,</span></a>
                        <span class="author-block"><a href="https://scholar.google.co.jp/citations?user=8lVpK1QAAAAJ&hl=zh-CN"
                     style="text-decoration: none; color: inherit;"><span class="author-block">Libo Qin,</span></a>
                        <span class="author-block">Fuwei Zhang,</span>
                        <span class="author-block"><span class="author-block"><a href="https://scholar.google.com/citations?user=cDcWXuIAAAAJ&hl=zh-CN"
                     style="text-decoration: none; color: inherit;"><span class="author-block">Lijuan Wang,</span></a>
                        <span class="author-block"><a href="https://scholar.google.com/citations?user=w47WJE4AAAAJ&hl=zh-CN"
                     style="text-decoration: none; color: inherit;"><span class="author-block">Min Li</span></a>
                     <br>
                    </div>
                    <br>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                  <span class="link-block">
                    <a href="https://arxiv.org/pdf/2502.07870"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>
                            <span class="link-block">
                    <a href="https://github.com/CSU-JPG/TextAtlas"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>

                <span class="link-block">
                    <a href="https://huggingface.co/datasets/CSU-JPG/TextAtlas5M" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon" style="font-size:18px">ü§ó</span>
                      <span>TextAtlas5M</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://huggingface.co/datasets/CSU-JPG/TextAtlasEval" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon" style="font-size:18px">ü§ó</span>
                      <span>TextAtlasEval</span>
                    </a>
                  </span>


                            <span class="link-block">
                    <a href="https://x.com/awinyimgprocess/status/1889910688627433725?s=46&t=HvOe9T2n35iFuCHP5aIHpQ&mx=2"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon has-text-white">
                        <i class="fa-brands fa-x-twitter"></i>
                      </span>
                      <span>Twitter</span>
                    </a>
                  </span>
                            <span class="link-block">
                    <a href="#leaderboard" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon has-text-white">
                        <i class="fa-solid fa-trophy"></i>
                      </span>
                      <span>Leaderboard</span>
                    </a>
                  </span>
                            <span class="link-block">
                    <a href="#examples" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon has-text-white">
                        <i class="fa-solid fa-book"></i>
                      </span>
                      <span>Examples</span>
                    </a>
                  </span>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="hero teaser">
    <div class="container is-max-desktop has-text-centered">
        <img src="./static/my_images/text_only_data-1.svg" alt="our observation">
        <div class="text-block">
        <p>Current text-to-image generation methods <span class="highlight"> struggle with rendering dense text</span>.
        
        For example, AnyText and Text Diffuser2, are capable of rendering short text but struggle with longer sequences. 
        GPT4o with DALLE-3 and SD3.5 Large show superior performance, though they still <span class="highlight"> produce inaccuracies such as duplicated words or missing letters when handling extended text</span>.
        For interleaved documents, all methods perform poorly due to their lack of layout planning capabilities.

        </br>
        <b>These results underscore that dense-text image generation remains a challenging task for current
            models.</b>
        </p>
    </div>
    </div>
</section>

<section class="section">
    <div class="container" style="margin-bottom: 2vh;">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">üîîNews</h2>
                <div class="content has-text-justified">
                    <p>
                        <b>üî•[2025-02-12] We introduce <a href="#">TextAtlas5M</a>, a dataset specifically designed for training and evaluating multimodal generation models on dense-text image generation. üöÄ</b>
                    </p>
                </div>
                </br>
                <h2 class="title is-3">Introduction</h2>
                <div class="content has-text-justified">
<!--                     <div class="text-block">
                    <p>
                        We present TextAtlas5M, <b>a large-scale dataset for text-conditioned
                        image generation, focusing on text-rich images. </b>The
                        dataset includes three levels of synthetic data and a
                        diverse collection of real images, curated from widely
                        relevant real-world topics.
                    </p>
                </div> -->

                <div class="text-block">
                    <p>
                        <span class="highlight">TextAtlas5M</span> focus on generating <span class="key-feature">dense-text images</span> and stands out in several key ways compared to previous text-rich datasets.
                        Unlike earlier datasets, which primarily focus on <span class="key-feature">short and simple text</span>, 
                        <span class="highlight">TextAtlas5M</span> includes a diverse and complex range of data. 
                        It spans from <span class="key-feature">interleaved documents</span> and <span class="key-feature">synthetic data</span> 
                        to <span class="key-feature">real-world images containing dense text</span>, offering a more varied 
                        and challenging set of examples. Moreover, our dataset features <span class="challenge">longer text captions</span>, 
                        which pose additional challenges for models, and includes <span class="challenge">human annotations</span> 
                        for particularly difficult examples, ensuring a more thorough evaluation of model capabilities.
                    </p>
                </div>
                    <img src="./static/my_images/compare.jpg" alt="algebraic reasoning" class="center" style="width: 100%; height: auto;">
                    <div class="text-block">
                    <p>
                        We design a dedicated test benchmark  <b>TextAtlasEval</b> to address the longstanding gap in metrics
                        for evaluating long-text information in image generation. By requiring models to effectively
                        process and
                        generate longer text, TextAtlas5M sets itself apart from
                        existing text rendering benchmarks.
                    </p>
                    <p>
                        We thoroughly
                        evaluate proprietary and open-source models to assess
                        their long-text generation capabilities. The results reveal the significant challenges posed by
                        TextAtlas5M
                        and provide valuable insights into the limitations of
                        current models, offering key directions for advancing
                        text-rich image generation in future research.
                    </p>
                </div>
                </div>
            </div>
        </div>

    </div>
</section>


<section class="hero is-light is-small">
    <div class="hero-body has-text-centered">
        <h1 class="title is-1 mmmu">

            <span class="mmmu">TextAtlas5M (Training Dataset)</span>
        </h1>
    </div>
</section>

<section class="section">
    <div class="container">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Overview</h2>
                <div class="content has-text-justified">
                   <img src="./static/my_images/intro_dataset-1.svg" alt="algebraic reasoning" class="center" style="width: 100%; height: auto;">
                    <br>
                <div class="text-block">
                    <h3>Synthetic Data</h3>
                    <h4>CleanTextSynth, TextVisionBlend, StyledTextSynth</h4>
                    <p>
                        The synthetic subset progresses through <span class="key-feature">three levels of complexity</span>, starting with 
                        <span class="key-feature">simple text</span> on clean backgrounds. It then advances to 
                        <span class="key-feature">interleaved data</span>, blending text with visual elements, and culminates in 
                        <span class="key-feature">synthetic natural images</span>, where realistic scenes integrate seamlessly with text.
                    </p>
                </div>


                <div class="text-block">
                    <h3>Real Data</h3>
                    <h4>The other subsets</h4>
                    <p>
                        The real image subset captures diverse, real-world <span class="key-feature">dense text scenarios</span>. It includes 
                        filtered samples from datasets like <span class="highlight">AnyText</span> and <span class="highlight">TextDiffuser</span>, 
                        detailed descriptions from PowerPoint slides, book covers, and academic PDF papers.
                    </p>
                    <p>
                        To enrich diversity, we also gather dense text images guided by predefined topics. 
                        To assess the capability of models in dense text image generation, we introduce a dedicated test set, 
                        <span class="highlight">TextAtlas5MEval</span>, designed for comprehensive evaluation.
                        This test set spans <span class="key-feature">four distinct data types</span>, ensuring diversity across domains 
                        and enhancing the relevance of <span class="highlight">TextAtlas5M</span> for real-world applications.
                    </p>
                </div>
                    <h2 class="title is-3" style="text-align: center;">Subset Example</h2>
                    <img src="./static/my_images/data-display-overall-w-ann-v2.svg" alt="algebraic reasoning" class="center" style="width: 100%; height: auto;">
                </div>
            </div>
        </div>

        <div class="columns is-centered m-6">
            <div class="column is-full has-text-centered content">
                <h2 class="title is-3">Statistics</h2>
                <div class="carousel results-carousel">
 <!--                    <div class="box m-5">
                        <div class="content has-text-centered">
                            <img src="./static/my_images/compare.jpg" alt="algebraic reasoning"
                                 width="95%"/>
                            <p> A comparison of TextAtlas5M with previous
                                text-rich image datasets. Unlike prior datasets with
                                short or unstructured alt-text, TextAtlas5M features more
                                diverse, complex long-form text.</p>
                        </div>
                    </div> -->
                    <div class="box m-5">
                        <div class="content has-text-centered">
                            <img src="./static/my_images/topic_pie_chart.png" alt="arithmetic reasoning" width="100%"
                                 height="100%"/>
                            <p> Topic distribution in StyledTextSynth and TextScenesHQ subset, showcasing a diverse
                                range of text-rich
                                topics such as weather reports, banners, and TV shopping ads. StyledTextSynth includes
                                carefully selected 18 topics, while
                                TextScenesHQ ultimately contains 26 distinct topics. These topics are generated using
                                GPT-4 as a world simulator and then
                                filtered by humans to eliminate overlap while ensuring diversity.</p>
                        </div>
                    </div>
                    <div class="box m-5">
                        <div class="content has-text-centered">
                            <img src="./static/my_images/compare2.jpg" alt="arithmetic reasoning" width="95%"/>
                            <p>Dataset Comparison with Existing Text-Rich Image Generation Datasets. The last two
                                columns detail the
                                sources of automatically generated labels, while the final column presents the average
                                text token length derived from OCR
                                applied to the images.</p>
                        </div>
                    </div>
                    <div class="box m-5">
                        <div class="content has-text-centered">
                            <img src="./static/my_images/perplexity.png" alt="arithmetic reasoning" width="100%"
                                 height="100%"/>
                            <p>
                                Kernel density estimations representing the distribution of perplexity scores for
                                TextAtlas5M compared to
                                reference datasets. The lower the perplexity for a document,
                                the more it resembles a Wikipedia article.
                            </p>
                        </div>
                    </div>
                    <div class="box m-5">
                        <div class="content has-text-centered">
                            <img src="./static/my_images/data_level.jpg" alt="arithmetic reasoning" width="100%"
                                 height="100%"/>
                            <p>
                                Data Level, Datasets, and Annotations Overview.
                            </p>
                        </div>
                    </div>
                    <div class="box m-5">
                        <div class="content has-text-centered">
                            <img src="./static/my_images/clip_score.png" alt="arithmetic reasoning" width="100%"
                                 height="100%"/>
                            <p>
                                CLIP Score Distribution.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>


<section class="hero is-light is-small">
    <div class="hero-body has-text-centered">
        <h1 class="title is-1 mmmu">
            <span class="mmmu">TextAtlasEval (Evaluate Benchmark)</span>
        </h1>
    </div>
</section>
<section class="section">
    <div class="container">
        <div class="content has-text-justified">
            <div class="text-block">
                <p>To evaluate the dense-text image generation ability for existing model, we further propose TextAtlas5MEval.
                    Adopting stratified random sampling weighted by subset complexity levels: 33% from advanced synthetic tiers
                    (Styled-TextSynth), 33% from real-world professional domains
                    TextScenesHQ, and 33% from web-sourced interleaved
                    TextVisionBlend coverage of both controlled and organic
                    scenarios.</p>
                <br>
                <p>
                    For the StyledTextSynth and TextScenesHQ subsets, we
                    sample data from each topic to ensure the evaluation set
                    covers a wide range of topics. For TextVisionBlend we perform random sampling to obtain the samples, and we
                    finally get a test set with 3000 samples. In this way, our
                    dataset cover different domain of data which allowing us to
                    assess the capabilities of model across multiple dimensions.
                </p>
            </div>
        </div>
    </div>
</section>


<!-- <section class="hero is-light is-small">
    <div class="hero-body has-text-centered">
        <h1 class="title is-1 mmmu">Experiment Results</h1>
    </div>
</section> -->

<section class="section">
    <div class="container">

        <div class="columns is-centered m-6">
            <div class="column is-full has-text-centered content">
                <h2 class="title is-3" id="leaderboard">Leaderboard</h2>
                <div class="content has-text-justified">
                    <p>
                        To further evaluate the model performance,
                        we evaluate FID score, CLIP Score, OCR accuracy
                        and show the results in the following table. We observe the SD-3.5
                        leads to the best result.
                    </p>
                </div>
                <br>
                <div class="content has-text-centered">
                    <p>
                        Long text image generation evaluation over TextAtlas5MEval. Metrics include F1 score (F1), CLIP Score (CS), and Character Error Rate (CER). 
                    </p>
                </div>
                <div class="leaderboard-container">
                    <style>
                        .grid-container {
                            display: grid;
                            grid-template-columns: 1fr; /* ‰∏§ÂàóÔºåÂπ≥ÂàÜÂÆΩÂ∫¶ */
                            grid-template-rows: auto auto auto auto; /* ‰∏§Ë°åÔºåËá™Âä®Ë∞ÉÊï¥È´òÂ∫¶ */
                            gap: 20px; /* Ë°®Ê†º‰πãÈó¥ÁöÑÈó¥Ë∑ù */
                            padding: 20px;
                        }

                        .grid-item {
                            border: 1px solid #ccc;
                            padding: 10px;
                            background-color: #f9f9f9;
                            box-shadow: 2px 2px 8px rgba(0, 0, 0, 0.1);
                        }

                        table {
                            width: 100%;
                            border-collapse: collapse;
                        }

                        th, td {
                            border: 1px solid #ddd;
                            padding: 8px;
                            text-align: center;
                        }

                        th {
                            background-color: #f2f2f2;
                        }

                        .table-title {
                            text-align: center;
                            font-weight: bold;
                            margin-bottom: 10px;
                        }
                    </style>

                    <div class="grid-container">
                        <!-- Â∑¶‰∏ä -->
                        <div class="grid-item">
                            <p class="table-title">Evaluation On TextVisionBlend</p>
                            <table id="leadertable-table1">
                                <thead>
                                <tr>
                                    <th>Method</th>
                                    <th>Date</th>
                                    <th>FID‚¨á</th>
                                    <th>CS‚¨Ü</th>
                                    <th>OCR(Acc.)‚¨Ü</th>
                                    <th>OCR(F1.)‚¨Ü</th>
                                    <th>OCR(Cer.)‚¨á</th>
                                </tr>
                                </thead>
                                <tr>
                                    <td>PixArt-Sigma</td>
                                    <td>2024-03-27</td>
                                    <td>81.29</td>
                                    <td>0.1891</td>
                                    <td>2.40</td>
                                    <td>1.57</td>
                                    <td>0.83</td>
                                </tr>
                                <tr>
                                    <td>Infinity-2B</td>
                                    <td>2024-12-05</td>
                                    <td>95.69</td>
                                    <td>0.1979</td>
                                    <td>2.98</td>
                                    <td>3.44</td>
                                    <td>0.83</td>
                                </tr>
                                <tr>
                                    <td>SD3.5 Large</td>
                                    <td>2024-06-25</td>
                                    <td>118.85</td>
                                    <td>0.1846</td>
                                    <td>14.55</td>
                                    <td>16.25</td>
                                    <td>0.88</td>
                                </tr>
                            </table>
                        </div>

<img src="./static/my_images/text_only_data_spcial_case_one_sample-1.svg" alt="arithmetic reasoning" width="100%" height="100%"/>

                        <!-- Âè≥‰∏ä -->
                        <div class="grid-item">
                            <p class="table-title">Evaluation On StyledTextSynth</p>
                            <table id="leadertable-table2">
                                <thead>
                                <tr>
                                    <th>Method</th>
                                    <th>Date</th>
                                    <th>FID‚¨á</th>
                                    <th>CS‚¨Ü</th>
                                    <th>OCR(Acc.)‚¨Ü</th>
                                    <th>OCR(F1.)‚¨Ü</th>
                                    <th>OCR(Cer.)‚¨á</th>
                                </tr>
                                </thead>
                                <tr>
                                    <td>SD3.5 Large</td>
                                    <td>2024-06-25</td>
                                    <td>71.09</td>
                                    <td>0.2849</td>
                                    <td>27.21</td>
                                    <td>33.86</td>
                                    <td>0.73</td>
                                </tr>
                                <tr>
                                    <td>PixArt-Sigma</td>
                                    <td>2024-03-27</td>
                                    <td>82.83</td>
                                    <td>0.2764</td>
                                    <td>0.42</td>
                                    <td>0.62</td>
                                    <td>0.90</td>
                                </tr>
                                <tr>
                                    <td>Infinity-2B</td>
                                    <td>2024-12-05</td>
                                    <td>84.95</td>
                                    <td>0.2727</td>
                                    <td>0.80</td>
                                    <td>1.42</td>
                                    <td>0.93</td>
                                </tr>
                                <tr>
                                    <td>Anytext</td>
                                    <td>2023-11-03</td>
                                    <td>117.71</td>
                                    <td>0.2501</td>
                                    <td>0.35</td>
                                    <td>0.66</td>
                                    <td>0.98</td>
                                </tr>
                                <tr>
                                    <td>TextDiffuser2</td>
                                    <td>2023-11-16</td>
                                    <td>114.31</td>
                                    <td>0.2510</td>
                                    <td>0.76</td>
                                    <td>1.46</td>
                                    <td>0.99</td>
                                </tr>
                            </table>
                        </div>
<img src="./static/my_images/text_only_data_mid-1.svg" alt="arithmetic reasoning" width="100%" height="100%"/>
                        <!-- Â∑¶‰∏ã -->
                        <div class="grid-item">
                            <p class="table-title">Evaluation On TextScenesHQ</p>
                            <table id="leadertable-table3">
                                <thead>
                                <tr>
                                    <th>Method</th>
                                    <th>Date</th>
                                    <th>FID‚¨á</th>
                                    <th>CS‚¨Ü</th>
                                    <th>OCR(Acc.)‚¨Ü</th>
                                    <th>OCR(F1.)‚¨Ü</th>
                                    <th>OCR(Cer.)‚¨á</th>
                                </tr>
                                </thead>
                                <tr>
                                    <td>SD3.5 Large</td>
                                    <td>2024-06-25</td>
                                    <td>64.44</td>
                                    <td>0.2363</td>
                                    <td>19.03</td>
                                    <td>24.45</td>
                                    <td>0.73</td>
                                </tr>
                                <tr>
                                    <td>Infinity-2B</td>
                                    <td>2024-12-05</td>
                                    <td>71.59</td>
                                    <td>0.2346</td>
                                    <td>1.06</td>
                                    <td>1.74</td>
                                    <td>0.88</td>
                                </tr>
                                <tr>
                                    <td>PixArt-Sigma</td>
                                    <td>2024-03-27</td>
                                    <td>72.62</td>
                                    <td>0.2347</td>
                                    <td>0.34</td>
                                    <td>0.53</td>
                                    <td>0.91</td>
                                </tr>
                                <tr>
                                    <td>TextDiffuser2</td>
                                    <td>2023-11-16</td>
                                    <td>84.10</td>
                                    <td>0.2252</td>
                                    <td>0.66</td>
                                    <td>1.25</td>
                                    <td>0.96</td>
                                </tr>
                                <tr>
                                    <td>Anytext</td>
                                    <td>2023-11-03</td>
                                    <td>101.32</td>
                                    <td>0.2174</td>
                                    <td>0.42</td>
                                    <td>0.80</td>
                                    <td>0.95</td>
                                </tr>
                            </table>
                        </div>
                    </div>

                </div>
            </div>
        </div>
 <!--        <div class="columns is-centered m-6">
            <div class="column is-full has-text-centered content">
                <h2 class="title is-3">Clip Score</h2>
                <div class="content has-text-justified">
                    <p>
                        we analyze image-text similarity by using text as queries and images as candidates. We utilize
                        the CLIP model to compute matching scores between text
                        and images. Specifically, we use the open-CLIP 4 implementation of the ViT-B-32 model trained on
                        the LAION-2B dataset. We
                        observe that the CLIP scores for LongWordsSubset-A,
                        LongWordsSubset-M, and Cover Book datasets are higher
                        compared to other subsets. Notably, these splits include
                        image captions, which likely contribute to the higher alignment.
                    </p>
                    </br>
                    <p>
                        In contrast, our generated interleaved data exhibits lower
                        matching scores. This suggests that the interleaved format
                        is less optimized for the CLIP model, presenting a challenge
                        for off-the-shelf models to effectively align text and images
                        in this format. Interestingly, for the Arxiv Paper subset, the
                        alignment scores are higher than those of the interleaved
                        data. This indicates that the CLIP model, trained on image-text data, possesses some OCR-like
                        capabilities, enabling it
                        to recognize and align textual elements within documents.
                    </p>

                </div>
                <div class="content has-text-centered">
                    <img src="./static/my_images/clip_score.png" alt="arithmetic reasoning"/>
                    <p class="bottom-text"> Clip score distribution over all sub datasets in
                        TextAtlas5M. We randomly sample 10k samples for each
                        subset in TextAtlas5M.</p>
                </div>

            </div>
        </div> -->

<section class="hero is-light is-small">
    <div class="hero-body has-text-centered">
        <h1 class="title is-1 mmmu">
            <span class="mmmu" id="examples">More Examples</span>
        </h1>
    </div>
</section>

        <div class="columns is-centered m-6">
            <div class="column is-full has-text-centered content">
                <div class="carousel results-carousel">
                    <!-- <div class="box m-5">
                        <div class="content has-text-centered">
                            <img src="./static/my_images/data-display-overall-w-ann-v2.svg" alt="grade-lv"
                                 width="100%"/>
                            <p class="bottom-text">Examples Overview</p>
                        </div>
                    </div> -->
                    <div class="box m-5">
                        <div class="content has-text-centered">
                            <img src="./static/my_images/long_image_4.jpg" alt="grade-lv" width="60%"/>
                            <p class="bottom-text">TextScenesHQ Examples_1</p>
                        </div>
                    </div>
                    <div class="box m-5">
                        <div class="content has-text-centered">
                            <img src="./static/my_images/long_image_5.jpg" alt="grade-lv" width="60%"/>
                            <p class="bottom-text">TextScenesHQ Examples_2</p>
                        </div>
                    </div>
                    <div class="box m-5">
                        <div class="content has-text-centered">
                            <img src="./static/my_images/long_image_6.jpg" alt="grade-lv" width="80%"/>
                            <p class="bottom-text">TextScenesHQ Examples_3</p>
                        </div>
                    </div>
                    <div class="box m-5">
                        <div class="content has-text-centered">
                            <img src="./static/my_images/long_image_1.jpg" alt="grade-lv" width="80%"/>
                            <p class="bottom-text">StyledTextSynth Examples_1</p>
                        </div>
                    </div>
                    <div class="box m-5">
                        <div class="content has-text-centered">
                            <img src="./static/my_images/long_image_2.jpg" alt="grade-lv" width="80%"/>
                            <p class="bottom-text">StyledTextSynth Examples_2</p>
                        </div>
                    </div>
                    <div class="box m-5">
                        <div class="content has-text-centered">
                            <img src="./static/my_images/long_image_3.jpg" alt="grade-lv" width="80%"/>
                            <p class="bottom-text">StyledTextSynth Examples_3</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>


<section class="hero is-light is-small">
    <div class="hero-body has-text-centered">
        <h1 class="title is-1 mmmu">
            <span class="mmmu">Potential Applications</span>
        </h1>
    </div>
</section>


<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <ul class="application-list">
            <li>üìå Interleaved PowerPoint/Wiki-Document Generation</li>
            <img src="./static/my_images/ppt_example.jpg" alt="grade-lv" width="40%" style="margin-right: 20px; margin-left: 20px;"/>
            <img src="./static/my_images/wiki_example.png" alt="grade-lv" width="35%"/>
            <li>üìå Text-customized Image Generation</li>
        </ul>
    </div>
</section>

<style>
    .application-list {
        list-style-type: none;
        padding-left: 0;
        font-size: 18px;
        margin-top: 10px;
    }
    .application-list li {
        padding: 8px 0;
        font-weight: bold;
        color: #333;
    }
</style>



<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title is-3 has-text-centered">BibTeX</h2>
        <pre><code>
        @inproceedings{wang2025large,
            title={A Large-scale Dataset for Dense Text Image Generation},
            author={Alex Jinpeng Wang and Dongxing Mao and  Jiawei Zhang and weiming Han and Zhuobai Dong and Linjie Li and Yiqi Lin and Zhengyuan Yang and Libo Qin and Fuwei Zhang and Lijuan Wang and Min Li},
            booktitle={arXiv preprint arXiv: 2502.07870},
            year={2025},
        }

    </code></pre>
    </div>
</section>

<footer class="footer">
    <div class="columns is-centered">
        <div class="column is-8">
            <div class="content">
                <p>
                    This website is website adapted from <a href="https://nerfies.github.io/">Nerfies</a> and <a
                        href="https://mathvista.github.io/">MathVista</a>, licensed under a <a rel="license"
                                                                                               href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                    Commons Attribution-ShareAlike 4.0 International License</a>.
                </p>
            </div>
        </div>
    </div>
</footer>

</body>
</html>
